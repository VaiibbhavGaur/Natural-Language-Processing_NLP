{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "peripheral-october",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "specified-ideal",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v2' has no attribute '__internal__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18592/1336207552.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# from keras.utils import to_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvolutional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvolutional_recurrent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\layers\\core\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivity_regularization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mActivityRegularization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\layers\\core\\dense.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minitializers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\initializers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;31m# from ALL_OBJECTS. We make no guarantees as to whether these objects will\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;31m# using their correct version.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m \u001b[0mpopulate_deserializable_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\initializers\\__init__.py\u001b[0m in \u001b[0;36mpopulate_deserializable_objects\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m   \u001b[0mLOCAL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m   \u001b[0mLOCAL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGENERATED_WITH_V2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;31m# Compatibility aliases (need to exist in both V1 and V2).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.compat.v2' has no attribute '__internal__'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "# from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-recorder",
   "metadata": {},
   "source": [
    "## Import and Analyse the imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "young-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "absolute-vacation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)   # Checking the size of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "welcome-introduction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data), len(training_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "concrete-impact",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_data), len(testing_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "encouraging-eligibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 194,\n",
       " 1153,\n",
       " 194,\n",
       " 8255,\n",
       " 78,\n",
       " 228,\n",
       " 5,\n",
       " 6,\n",
       " 1463,\n",
       " 4369,\n",
       " 5012,\n",
       " 134,\n",
       " 26,\n",
       " 4,\n",
       " 715,\n",
       " 8,\n",
       " 118,\n",
       " 1634,\n",
       " 14,\n",
       " 394,\n",
       " 20,\n",
       " 13,\n",
       " 119,\n",
       " 954,\n",
       " 189,\n",
       " 102,\n",
       " 5,\n",
       " 207,\n",
       " 110,\n",
       " 3103,\n",
       " 21,\n",
       " 14,\n",
       " 69,\n",
       " 188,\n",
       " 8,\n",
       " 30,\n",
       " 23,\n",
       " 7,\n",
       " 4,\n",
       " 249,\n",
       " 126,\n",
       " 93,\n",
       " 4,\n",
       " 114,\n",
       " 9,\n",
       " 2300,\n",
       " 1523,\n",
       " 5,\n",
       " 647,\n",
       " 4,\n",
       " 116,\n",
       " 9,\n",
       " 35,\n",
       " 8163,\n",
       " 4,\n",
       " 229,\n",
       " 9,\n",
       " 340,\n",
       " 1322,\n",
       " 4,\n",
       " 118,\n",
       " 9,\n",
       " 4,\n",
       " 130,\n",
       " 4901,\n",
       " 19,\n",
       " 4,\n",
       " 1002,\n",
       " 5,\n",
       " 89,\n",
       " 29,\n",
       " 952,\n",
       " 46,\n",
       " 37,\n",
       " 4,\n",
       " 455,\n",
       " 9,\n",
       " 45,\n",
       " 43,\n",
       " 38,\n",
       " 1543,\n",
       " 1905,\n",
       " 398,\n",
       " 4,\n",
       " 1649,\n",
       " 26,\n",
       " 6853,\n",
       " 5,\n",
       " 163,\n",
       " 11,\n",
       " 3215,\n",
       " 2,\n",
       " 4,\n",
       " 1153,\n",
       " 9,\n",
       " 194,\n",
       " 775,\n",
       " 7,\n",
       " 8255,\n",
       " 2,\n",
       " 349,\n",
       " 2637,\n",
       " 148,\n",
       " 605,\n",
       " 2,\n",
       " 8003,\n",
       " 15,\n",
       " 123,\n",
       " 125,\n",
       " 68,\n",
       " 2,\n",
       " 6853,\n",
       " 15,\n",
       " 349,\n",
       " 165,\n",
       " 4362,\n",
       " 98,\n",
       " 5,\n",
       " 4,\n",
       " 228,\n",
       " 9,\n",
       " 43,\n",
       " 2,\n",
       " 1157,\n",
       " 15,\n",
       " 299,\n",
       " 120,\n",
       " 5,\n",
       " 120,\n",
       " 174,\n",
       " 11,\n",
       " 220,\n",
       " 175,\n",
       " 136,\n",
       " 50,\n",
       " 9,\n",
       " 4373,\n",
       " 228,\n",
       " 8255,\n",
       " 5,\n",
       " 2,\n",
       " 656,\n",
       " 245,\n",
       " 2350,\n",
       " 5,\n",
       " 4,\n",
       " 9837,\n",
       " 131,\n",
       " 152,\n",
       " 491,\n",
       " 18,\n",
       " 2,\n",
       " 32,\n",
       " 7464,\n",
       " 1212,\n",
       " 14,\n",
       " 9,\n",
       " 6,\n",
       " 371,\n",
       " 78,\n",
       " 22,\n",
       " 625,\n",
       " 64,\n",
       " 1382,\n",
       " 9,\n",
       " 8,\n",
       " 168,\n",
       " 145,\n",
       " 23,\n",
       " 4,\n",
       " 1690,\n",
       " 15,\n",
       " 16,\n",
       " 4,\n",
       " 1355,\n",
       " 5,\n",
       " 28,\n",
       " 6,\n",
       " 52,\n",
       " 154,\n",
       " 462,\n",
       " 33,\n",
       " 89,\n",
       " 78,\n",
       " 285,\n",
       " 16,\n",
       " 145,\n",
       " 95]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "forty-persian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: [0 1]\n",
      "Number of unique words or Vocab Size: 9998\n"
     ]
    }
   ],
   "source": [
    "print(\"Categories:\", np.unique(targets))\n",
    "print(\"Number of unique words or Vocab Size:\", len(np.unique(np.hstack(data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "offshore-arthritis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Review length: 234.75892\n",
      "Standard Deviation: 173\n"
     ]
    }
   ],
   "source": [
    "# Calculating the average and standard deviation of lenghths all reviews \n",
    "length = [len(i) for i in data]\n",
    "print(\"Average Review length:\", np.mean(length))\n",
    "print(\"Standard Deviation:\", round(np.std(length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "satellite-poker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# print the specific review and its label\n",
    "print(\"Review:\", data[2])\n",
    "print(\"Label:\", targets[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "closing-enforcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for # and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also # to the two little boy's that played the # of norman and paul they were just brilliant children are often left out of the # list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# Dictionary mapping word indices back into the original words so that we can read them. \n",
    "# It replaces every unknown word with a “#”. \n",
    "# It does this by using the get_word_index() function.\n",
    "\n",
    "index = imdb.get_word_index()\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
    "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[0]] )\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "conceptual-gathering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{34701: 'fawn',\n",
       " 52006: 'tsukino',\n",
       " 52007: 'nunnery',\n",
       " 16816: 'sonja',\n",
       " 63951: 'vani',\n",
       " 1408: 'woods',\n",
       " 16115: 'spiders',\n",
       " 2345: 'hanging',\n",
       " 2289: 'woody',\n",
       " 52008: 'trawling',\n",
       " 52009: \"hold's\",\n",
       " 11307: 'comically',\n",
       " 40830: 'localized',\n",
       " 30568: 'disobeying',\n",
       " 52010: \"'royale\",\n",
       " 40831: \"harpo's\",\n",
       " 52011: 'canet',\n",
       " 19313: 'aileen',\n",
       " 52012: 'acurately',\n",
       " 52013: \"diplomat's\",\n",
       " 25242: 'rickman',\n",
       " 6746: 'arranged',\n",
       " 52014: 'rumbustious',\n",
       " 52015: 'familiarness',\n",
       " 52016: \"spider'\",\n",
       " 68804: 'hahahah',\n",
       " 52017: \"wood'\",\n",
       " 40833: 'transvestism',\n",
       " 34702: \"hangin'\",\n",
       " 2338: 'bringing',\n",
       " 40834: 'seamier',\n",
       " 34703: 'wooded',\n",
       " 52018: 'bravora',\n",
       " 16817: 'grueling',\n",
       " 1636: 'wooden',\n",
       " 16818: 'wednesday',\n",
       " 52019: \"'prix\",\n",
       " 34704: 'altagracia',\n",
       " 52020: 'circuitry',\n",
       " 11585: 'crotch',\n",
       " 57766: 'busybody',\n",
       " 52021: \"tart'n'tangy\",\n",
       " 14129: 'burgade',\n",
       " 52023: 'thrace',\n",
       " 11038: \"tom's\",\n",
       " 52025: 'snuggles',\n",
       " 29114: 'francesco',\n",
       " 52027: 'complainers',\n",
       " 52125: 'templarios',\n",
       " 40835: '272',\n",
       " 52028: '273',\n",
       " 52130: 'zaniacs',\n",
       " 34706: '275',\n",
       " 27631: 'consenting',\n",
       " 40836: 'snuggled',\n",
       " 15492: 'inanimate',\n",
       " 52030: 'uality',\n",
       " 11926: 'bronte',\n",
       " 4010: 'errors',\n",
       " 3230: 'dialogs',\n",
       " 52031: \"yomada's\",\n",
       " 34707: \"madman's\",\n",
       " 30585: 'dialoge',\n",
       " 52033: 'usenet',\n",
       " 40837: 'videodrome',\n",
       " 26338: \"kid'\",\n",
       " 52034: 'pawed',\n",
       " 30569: \"'girlfriend'\",\n",
       " 52035: \"'pleasure\",\n",
       " 52036: \"'reloaded'\",\n",
       " 40839: \"kazakos'\",\n",
       " 52037: 'rocque',\n",
       " 52038: 'mailings',\n",
       " 11927: 'brainwashed',\n",
       " 16819: 'mcanally',\n",
       " 52039: \"tom''\",\n",
       " 25243: 'kurupt',\n",
       " 21905: 'affiliated',\n",
       " 52040: 'babaganoosh',\n",
       " 40840: \"noe's\",\n",
       " 40841: 'quart',\n",
       " 359: 'kids',\n",
       " 5034: 'uplifting',\n",
       " 7093: 'controversy',\n",
       " 21906: 'kida',\n",
       " 23379: 'kidd',\n",
       " 52041: \"error'\",\n",
       " 52042: 'neurologist',\n",
       " 18510: 'spotty',\n",
       " 30570: 'cobblers',\n",
       " 9878: 'projection',\n",
       " 40842: 'fastforwarding',\n",
       " 52043: 'sters',\n",
       " 52044: \"eggar's\",\n",
       " 52045: 'etherything',\n",
       " 40843: 'gateshead',\n",
       " 34708: 'airball',\n",
       " 25244: 'unsinkable',\n",
       " 7180: 'stern',\n",
       " 52046: \"cervi's\",\n",
       " 40844: 'dnd',\n",
       " 11586: 'dna',\n",
       " 20598: 'insecurity',\n",
       " 52047: \"'reboot'\",\n",
       " 11037: 'trelkovsky',\n",
       " 52048: 'jaekel',\n",
       " 52049: 'sidebars',\n",
       " 52050: \"sforza's\",\n",
       " 17633: 'distortions',\n",
       " 52051: 'mutinies',\n",
       " 30602: 'sermons',\n",
       " 40846: '7ft',\n",
       " 52052: 'boobage',\n",
       " 52053: \"o'bannon's\",\n",
       " 23380: 'populations',\n",
       " 52054: 'chulak',\n",
       " 27633: 'mesmerize',\n",
       " 52055: 'quinnell',\n",
       " 10307: 'yahoo',\n",
       " 52057: 'meteorologist',\n",
       " 42577: 'beswick',\n",
       " 15493: 'boorman',\n",
       " 40847: 'voicework',\n",
       " 52058: \"ster'\",\n",
       " 22922: 'blustering',\n",
       " 52059: 'hj',\n",
       " 27634: 'intake',\n",
       " 5621: 'morally',\n",
       " 40849: 'jumbling',\n",
       " 52060: 'bowersock',\n",
       " 52061: \"'porky's'\",\n",
       " 16821: 'gershon',\n",
       " 40850: 'ludicrosity',\n",
       " 52062: 'coprophilia',\n",
       " 40851: 'expressively',\n",
       " 19500: \"india's\",\n",
       " 34710: \"post's\",\n",
       " 52063: 'wana',\n",
       " 5283: 'wang',\n",
       " 30571: 'wand',\n",
       " 25245: 'wane',\n",
       " 52321: 'edgeways',\n",
       " 34711: 'titanium',\n",
       " 40852: 'pinta',\n",
       " 178: 'want',\n",
       " 30572: 'pinto',\n",
       " 52065: 'whoopdedoodles',\n",
       " 21908: 'tchaikovsky',\n",
       " 2103: 'travel',\n",
       " 52066: \"'victory'\",\n",
       " 11928: 'copious',\n",
       " 22433: 'gouge',\n",
       " 52067: \"chapters'\",\n",
       " 6702: 'barbra',\n",
       " 30573: 'uselessness',\n",
       " 52068: \"wan'\",\n",
       " 27635: 'assimilated',\n",
       " 16116: 'petiot',\n",
       " 52069: 'most\\x85and',\n",
       " 3930: 'dinosaurs',\n",
       " 352: 'wrong',\n",
       " 52070: 'seda',\n",
       " 52071: 'stollen',\n",
       " 34712: 'sentencing',\n",
       " 40853: 'ouroboros',\n",
       " 40854: 'assimilates',\n",
       " 40855: 'colorfully',\n",
       " 27636: 'glenne',\n",
       " 52072: 'dongen',\n",
       " 4760: 'subplots',\n",
       " 52073: 'kiloton',\n",
       " 23381: 'chandon',\n",
       " 34713: \"effect'\",\n",
       " 27637: 'snugly',\n",
       " 40856: 'kuei',\n",
       " 9092: 'welcomed',\n",
       " 30071: 'dishonor',\n",
       " 52075: 'concurrence',\n",
       " 23382: 'stoicism',\n",
       " 14896: \"guys'\",\n",
       " 52077: \"beroemd'\",\n",
       " 6703: 'butcher',\n",
       " 40857: \"melfi's\",\n",
       " 30623: 'aargh',\n",
       " 20599: 'playhouse',\n",
       " 11308: 'wickedly',\n",
       " 1180: 'fit',\n",
       " 52078: 'labratory',\n",
       " 40859: 'lifeline',\n",
       " 1927: 'screaming',\n",
       " 4287: 'fix',\n",
       " 52079: 'cineliterate',\n",
       " 52080: 'fic',\n",
       " 52081: 'fia',\n",
       " 34714: 'fig',\n",
       " 52082: 'fmvs',\n",
       " 52083: 'fie',\n",
       " 52084: 'reentered',\n",
       " 30574: 'fin',\n",
       " 52085: 'doctresses',\n",
       " 52086: 'fil',\n",
       " 12606: 'zucker',\n",
       " 31931: 'ached',\n",
       " 52088: 'counsil',\n",
       " 52089: 'paterfamilias',\n",
       " 13885: 'songwriter',\n",
       " 34715: 'shivam',\n",
       " 9654: 'hurting',\n",
       " 299: 'effects',\n",
       " 52090: 'slauther',\n",
       " 52091: \"'flame'\",\n",
       " 52092: 'sommerset',\n",
       " 52093: 'interwhined',\n",
       " 27638: 'whacking',\n",
       " 52094: 'bartok',\n",
       " 8775: 'barton',\n",
       " 21909: 'frewer',\n",
       " 52095: \"fi'\",\n",
       " 6192: 'ingrid',\n",
       " 30575: 'stribor',\n",
       " 52096: 'approporiately',\n",
       " 52097: 'wobblyhand',\n",
       " 52098: 'tantalisingly',\n",
       " 52099: 'ankylosaurus',\n",
       " 17634: 'parasites',\n",
       " 52100: 'childen',\n",
       " 52101: \"jenkins'\",\n",
       " 52102: 'metafiction',\n",
       " 17635: 'golem',\n",
       " 40860: 'indiscretion',\n",
       " 23383: \"reeves'\",\n",
       " 57781: \"inamorata's\",\n",
       " 52104: 'brittannica',\n",
       " 7916: 'adapt',\n",
       " 30576: \"russo's\",\n",
       " 48246: 'guitarists',\n",
       " 10553: 'abbott',\n",
       " 40861: 'abbots',\n",
       " 17649: 'lanisha',\n",
       " 40863: 'magickal',\n",
       " 52105: 'mattter',\n",
       " 52106: \"'willy\",\n",
       " 34716: 'pumpkins',\n",
       " 52107: 'stuntpeople',\n",
       " 30577: 'estimate',\n",
       " 40864: 'ugghhh',\n",
       " 11309: 'gameplay',\n",
       " 52108: \"wern't\",\n",
       " 40865: \"n'sync\",\n",
       " 16117: 'sickeningly',\n",
       " 40866: 'chiara',\n",
       " 4011: 'disturbed',\n",
       " 40867: 'portmanteau',\n",
       " 52109: 'ineffectively',\n",
       " 82143: \"duchonvey's\",\n",
       " 37519: \"nasty'\",\n",
       " 1285: 'purpose',\n",
       " 52112: 'lazers',\n",
       " 28105: 'lightened',\n",
       " 52113: 'kaliganj',\n",
       " 52114: 'popularism',\n",
       " 18511: \"damme's\",\n",
       " 30578: 'stylistics',\n",
       " 52115: 'mindgaming',\n",
       " 46449: 'spoilerish',\n",
       " 52117: \"'corny'\",\n",
       " 34718: 'boerner',\n",
       " 6792: 'olds',\n",
       " 52118: 'bakelite',\n",
       " 27639: 'renovated',\n",
       " 27640: 'forrester',\n",
       " 52119: \"lumiere's\",\n",
       " 52024: 'gaskets',\n",
       " 884: 'needed',\n",
       " 34719: 'smight',\n",
       " 1297: 'master',\n",
       " 25905: \"edie's\",\n",
       " 40868: 'seeber',\n",
       " 52120: 'hiya',\n",
       " 52121: 'fuzziness',\n",
       " 14897: 'genesis',\n",
       " 12607: 'rewards',\n",
       " 30579: 'enthrall',\n",
       " 40869: \"'about\",\n",
       " 52122: \"recollection's\",\n",
       " 11039: 'mutilated',\n",
       " 52123: 'fatherlands',\n",
       " 52124: \"fischer's\",\n",
       " 5399: 'positively',\n",
       " 34705: '270',\n",
       " 34720: 'ahmed',\n",
       " 9836: 'zatoichi',\n",
       " 13886: 'bannister',\n",
       " 52127: 'anniversaries',\n",
       " 30580: \"helm's\",\n",
       " 52128: \"'work'\",\n",
       " 34721: 'exclaimed',\n",
       " 52129: \"'unfunny'\",\n",
       " 52029: '274',\n",
       " 544: 'feeling',\n",
       " 52131: \"wanda's\",\n",
       " 33266: 'dolan',\n",
       " 52133: '278',\n",
       " 52134: 'peacoat',\n",
       " 40870: 'brawny',\n",
       " 40871: 'mishra',\n",
       " 40872: 'worlders',\n",
       " 52135: 'protags',\n",
       " 52136: 'skullcap',\n",
       " 57596: 'dastagir',\n",
       " 5622: 'affairs',\n",
       " 7799: 'wholesome',\n",
       " 52137: 'hymen',\n",
       " 25246: 'paramedics',\n",
       " 52138: 'unpersons',\n",
       " 52139: 'heavyarms',\n",
       " 52140: 'affaire',\n",
       " 52141: 'coulisses',\n",
       " 40873: 'hymer',\n",
       " 52142: 'kremlin',\n",
       " 30581: 'shipments',\n",
       " 52143: 'pixilated',\n",
       " 30582: \"'00s\",\n",
       " 18512: 'diminishing',\n",
       " 1357: 'cinematic',\n",
       " 14898: 'resonates',\n",
       " 40874: 'simplify',\n",
       " 40875: \"nature'\",\n",
       " 40876: 'temptresses',\n",
       " 16822: 'reverence',\n",
       " 19502: 'resonated',\n",
       " 34722: 'dailey',\n",
       " 52144: '2\\x85',\n",
       " 27641: 'treize',\n",
       " 52145: 'majo',\n",
       " 21910: 'kiya',\n",
       " 52146: 'woolnough',\n",
       " 39797: 'thanatos',\n",
       " 35731: 'sandoval',\n",
       " 40879: 'dorama',\n",
       " 52147: \"o'shaughnessy\",\n",
       " 4988: 'tech',\n",
       " 32018: 'fugitives',\n",
       " 30583: 'teck',\n",
       " 76125: \"'e'\",\n",
       " 40881: 'doesn’t',\n",
       " 52149: 'purged',\n",
       " 657: 'saying',\n",
       " 41095: \"martians'\",\n",
       " 23418: 'norliss',\n",
       " 27642: 'dickey',\n",
       " 52152: 'dicker',\n",
       " 52153: \"'sependipity\",\n",
       " 8422: 'padded',\n",
       " 57792: 'ordell',\n",
       " 40882: \"sturges'\",\n",
       " 52154: 'independentcritics',\n",
       " 5745: 'tempted',\n",
       " 34724: \"atkinson's\",\n",
       " 25247: 'hounded',\n",
       " 52155: 'apace',\n",
       " 15494: 'clicked',\n",
       " 30584: \"'humor'\",\n",
       " 17177: \"martino's\",\n",
       " 52156: \"'supporting\",\n",
       " 52032: 'warmongering',\n",
       " 34725: \"zemeckis's\",\n",
       " 21911: 'lube',\n",
       " 52157: 'shocky',\n",
       " 7476: 'plate',\n",
       " 40883: 'plata',\n",
       " 40884: 'sturgess',\n",
       " 40885: \"nerds'\",\n",
       " 20600: 'plato',\n",
       " 34726: 'plath',\n",
       " 40886: 'platt',\n",
       " 52159: 'mcnab',\n",
       " 27643: 'clumsiness',\n",
       " 3899: 'altogether',\n",
       " 42584: 'massacring',\n",
       " 52160: 'bicenntinial',\n",
       " 40887: 'skaal',\n",
       " 14360: 'droning',\n",
       " 8776: 'lds',\n",
       " 21912: 'jaguar',\n",
       " 34727: \"cale's\",\n",
       " 1777: 'nicely',\n",
       " 4588: 'mummy',\n",
       " 18513: \"lot's\",\n",
       " 10086: 'patch',\n",
       " 50202: 'kerkhof',\n",
       " 52161: \"leader's\",\n",
       " 27644: \"'movie\",\n",
       " 52162: 'uncomfirmed',\n",
       " 40888: 'heirloom',\n",
       " 47360: 'wrangle',\n",
       " 52163: 'emotion\\x85',\n",
       " 52164: \"'stargate'\",\n",
       " 40889: 'pinoy',\n",
       " 40890: 'conchatta',\n",
       " 41128: 'broeke',\n",
       " 40891: 'advisedly',\n",
       " 17636: \"barker's\",\n",
       " 52166: 'descours',\n",
       " 772: 'lots',\n",
       " 9259: 'lotr',\n",
       " 9879: 'irs',\n",
       " 52167: 'lott',\n",
       " 40892: 'xvi',\n",
       " 34728: 'irk',\n",
       " 52168: 'irl',\n",
       " 6887: 'ira',\n",
       " 21913: 'belzer',\n",
       " 52169: 'irc',\n",
       " 27645: 'ire',\n",
       " 40893: 'requisites',\n",
       " 7693: 'discipline',\n",
       " 52961: 'lyoko',\n",
       " 11310: 'extend',\n",
       " 873: 'nature',\n",
       " 52170: \"'dickie'\",\n",
       " 40894: 'optimist',\n",
       " 30586: 'lapping',\n",
       " 3900: 'superficial',\n",
       " 52171: 'vestment',\n",
       " 2823: 'extent',\n",
       " 52172: 'tendons',\n",
       " 52173: \"heller's\",\n",
       " 52174: 'quagmires',\n",
       " 52175: 'miyako',\n",
       " 20601: 'moocow',\n",
       " 52176: \"coles'\",\n",
       " 40895: 'lookit',\n",
       " 52177: 'ravenously',\n",
       " 40896: 'levitating',\n",
       " 52178: 'perfunctorily',\n",
       " 30587: 'lookin',\n",
       " 40898: \"lot'\",\n",
       " 52179: 'lookie',\n",
       " 34870: 'fearlessly',\n",
       " 52181: 'libyan',\n",
       " 40899: 'fondles',\n",
       " 35714: 'gopher',\n",
       " 40901: 'wearying',\n",
       " 52182: \"nz's\",\n",
       " 27646: 'minuses',\n",
       " 52183: 'puposelessly',\n",
       " 52184: 'shandling',\n",
       " 31268: 'decapitates',\n",
       " 11929: 'humming',\n",
       " 40902: \"'nother\",\n",
       " 21914: 'smackdown',\n",
       " 30588: 'underdone',\n",
       " 40903: 'frf',\n",
       " 52185: 'triviality',\n",
       " 25248: 'fro',\n",
       " 8777: 'bothers',\n",
       " 52186: \"'kensington\",\n",
       " 73: 'much',\n",
       " 34730: 'muco',\n",
       " 22615: 'wiseguy',\n",
       " 27648: \"richie's\",\n",
       " 40904: 'tonino',\n",
       " 52187: 'unleavened',\n",
       " 11587: 'fry',\n",
       " 40905: \"'tv'\",\n",
       " 40906: 'toning',\n",
       " 14361: 'obese',\n",
       " 30589: 'sensationalized',\n",
       " 40907: 'spiv',\n",
       " 6259: 'spit',\n",
       " 7364: 'arkin',\n",
       " 21915: 'charleton',\n",
       " 16823: 'jeon',\n",
       " 21916: 'boardroom',\n",
       " 4989: 'doubts',\n",
       " 3084: 'spin',\n",
       " 53083: 'hepo',\n",
       " 27649: 'wildcat',\n",
       " 10584: 'venoms',\n",
       " 52191: 'misconstrues',\n",
       " 18514: 'mesmerising',\n",
       " 40908: 'misconstrued',\n",
       " 52192: 'rescinds',\n",
       " 52193: 'prostrate',\n",
       " 40909: 'majid',\n",
       " 16479: 'climbed',\n",
       " 34731: 'canoeing',\n",
       " 52195: 'majin',\n",
       " 57804: 'animie',\n",
       " 40910: 'sylke',\n",
       " 14899: 'conditioned',\n",
       " 40911: 'waddell',\n",
       " 52196: '3\\x85',\n",
       " 41188: 'hyperdrive',\n",
       " 34732: 'conditioner',\n",
       " 53153: 'bricklayer',\n",
       " 2576: 'hong',\n",
       " 52198: 'memoriam',\n",
       " 30592: 'inventively',\n",
       " 25249: \"levant's\",\n",
       " 20638: 'portobello',\n",
       " 52200: 'remand',\n",
       " 19504: 'mummified',\n",
       " 27650: 'honk',\n",
       " 19505: 'spews',\n",
       " 40912: 'visitations',\n",
       " 52201: 'mummifies',\n",
       " 25250: 'cavanaugh',\n",
       " 23385: 'zeon',\n",
       " 40913: \"jungle's\",\n",
       " 34733: 'viertel',\n",
       " 27651: 'frenchmen',\n",
       " 52202: 'torpedoes',\n",
       " 52203: 'schlessinger',\n",
       " 34734: 'torpedoed',\n",
       " 69876: 'blister',\n",
       " 52204: 'cinefest',\n",
       " 34735: 'furlough',\n",
       " 52205: 'mainsequence',\n",
       " 40914: 'mentors',\n",
       " 9094: 'academic',\n",
       " 20602: 'stillness',\n",
       " 40915: 'academia',\n",
       " 52206: 'lonelier',\n",
       " 52207: 'nibby',\n",
       " 52208: \"losers'\",\n",
       " 40916: 'cineastes',\n",
       " 4449: 'corporate',\n",
       " 40917: 'massaging',\n",
       " 30593: 'bellow',\n",
       " 19506: 'absurdities',\n",
       " 53241: 'expetations',\n",
       " 40918: 'nyfiken',\n",
       " 75638: 'mehras',\n",
       " 52209: 'lasse',\n",
       " 52210: 'visability',\n",
       " 33946: 'militarily',\n",
       " 52211: \"elder'\",\n",
       " 19023: 'gainsbourg',\n",
       " 20603: 'hah',\n",
       " 13420: 'hai',\n",
       " 34736: 'haj',\n",
       " 25251: 'hak',\n",
       " 4311: 'hal',\n",
       " 4892: 'ham',\n",
       " 53259: 'duffer',\n",
       " 52213: 'haa',\n",
       " 66: 'had',\n",
       " 11930: 'advancement',\n",
       " 16825: 'hag',\n",
       " 25252: \"hand'\",\n",
       " 13421: 'hay',\n",
       " 20604: 'mcnamara',\n",
       " 52214: \"mozart's\",\n",
       " 30731: 'duffel',\n",
       " 30594: 'haq',\n",
       " 13887: 'har',\n",
       " 44: 'has',\n",
       " 2401: 'hat',\n",
       " 40919: 'hav',\n",
       " 30595: 'haw',\n",
       " 52215: 'figtings',\n",
       " 15495: 'elders',\n",
       " 52216: 'underpanted',\n",
       " 52217: 'pninson',\n",
       " 27652: 'unequivocally',\n",
       " 23673: \"barbara's\",\n",
       " 52219: \"bello'\",\n",
       " 12997: 'indicative',\n",
       " 40920: 'yawnfest',\n",
       " 52220: 'hexploitation',\n",
       " 52221: \"loder's\",\n",
       " 27653: 'sleuthing',\n",
       " 32622: \"justin's\",\n",
       " 52222: \"'ball\",\n",
       " 52223: \"'summer\",\n",
       " 34935: \"'demons'\",\n",
       " 52225: \"mormon's\",\n",
       " 34737: \"laughton's\",\n",
       " 52226: 'debell',\n",
       " 39724: 'shipyard',\n",
       " 30597: 'unabashedly',\n",
       " 40401: 'disks',\n",
       " 2290: 'crowd',\n",
       " 10087: 'crowe',\n",
       " 56434: \"vancouver's\",\n",
       " 34738: 'mosques',\n",
       " 6627: 'crown',\n",
       " 52227: 'culpas',\n",
       " 27654: 'crows',\n",
       " 53344: 'surrell',\n",
       " 52229: 'flowless',\n",
       " 52230: 'sheirk',\n",
       " 40923: \"'three\",\n",
       " 52231: \"peterson'\",\n",
       " 52232: 'ooverall',\n",
       " 40924: 'perchance',\n",
       " 1321: 'bottom',\n",
       " 53363: 'chabert',\n",
       " 52233: 'sneha',\n",
       " 13888: 'inhuman',\n",
       " 52234: 'ichii',\n",
       " 52235: 'ursla',\n",
       " 30598: 'completly',\n",
       " 40925: 'moviedom',\n",
       " 52236: 'raddick',\n",
       " 51995: 'brundage',\n",
       " 40926: 'brigades',\n",
       " 1181: 'starring',\n",
       " 52237: \"'goal'\",\n",
       " 52238: 'caskets',\n",
       " 52239: 'willcock',\n",
       " 52240: \"threesome's\",\n",
       " 52241: \"mosque'\",\n",
       " 52242: \"cover's\",\n",
       " 17637: 'spaceships',\n",
       " 40927: 'anomalous',\n",
       " 27655: 'ptsd',\n",
       " 52243: 'shirdan',\n",
       " 21962: 'obscenity',\n",
       " 30599: 'lemmings',\n",
       " 30600: 'duccio',\n",
       " 52244: \"levene's\",\n",
       " 52245: \"'gorby'\",\n",
       " 25255: \"teenager's\",\n",
       " 5340: 'marshall',\n",
       " 9095: 'honeymoon',\n",
       " 3231: 'shoots',\n",
       " 12258: 'despised',\n",
       " 52246: 'okabasho',\n",
       " 8289: 'fabric',\n",
       " 18515: 'cannavale',\n",
       " 3537: 'raped',\n",
       " 52247: \"tutt's\",\n",
       " 17638: 'grasping',\n",
       " 18516: 'despises',\n",
       " 40928: \"thief's\",\n",
       " 8926: 'rapes',\n",
       " 52248: 'raper',\n",
       " 27656: \"eyre'\",\n",
       " 52249: 'walchek',\n",
       " 23386: \"elmo's\",\n",
       " 40929: 'perfumes',\n",
       " 21918: 'spurting',\n",
       " 52250: \"exposition'\\x85\",\n",
       " 52251: 'denoting',\n",
       " 34740: 'thesaurus',\n",
       " 40930: \"shoot'\",\n",
       " 49759: 'bonejack',\n",
       " 52253: 'simpsonian',\n",
       " 30601: 'hebetude',\n",
       " 34741: \"hallow's\",\n",
       " 52254: 'desperation\\x85',\n",
       " 34742: 'incinerator',\n",
       " 10308: 'congratulations',\n",
       " 52255: 'humbled',\n",
       " 5924: \"else's\",\n",
       " 40845: 'trelkovski',\n",
       " 52256: \"rape'\",\n",
       " 59386: \"'chapters'\",\n",
       " 52257: '1600s',\n",
       " 7253: 'martian',\n",
       " 25256: 'nicest',\n",
       " 52259: 'eyred',\n",
       " 9457: 'passenger',\n",
       " 6041: 'disgrace',\n",
       " 52260: 'moderne',\n",
       " 5120: 'barrymore',\n",
       " 52261: 'yankovich',\n",
       " 40931: 'moderns',\n",
       " 52262: 'studliest',\n",
       " 52263: 'bedsheet',\n",
       " 14900: 'decapitation',\n",
       " 52264: 'slurring',\n",
       " 52265: \"'nunsploitation'\",\n",
       " 34743: \"'character'\",\n",
       " 9880: 'cambodia',\n",
       " 52266: 'rebelious',\n",
       " 27657: 'pasadena',\n",
       " 40932: 'crowne',\n",
       " 52267: \"'bedchamber\",\n",
       " 52268: 'conjectural',\n",
       " 52269: 'appologize',\n",
       " 52270: 'halfassing',\n",
       " 57816: 'paycheque',\n",
       " 20606: 'palms',\n",
       " 52271: \"'islands\",\n",
       " 40933: 'hawked',\n",
       " 21919: 'palme',\n",
       " 40934: 'conservatively',\n",
       " 64007: 'larp',\n",
       " 5558: 'palma',\n",
       " 21920: 'smelling',\n",
       " 12998: 'aragorn',\n",
       " 52272: 'hawker',\n",
       " 52273: 'hawkes',\n",
       " 3975: 'explosions',\n",
       " 8059: 'loren',\n",
       " 52274: \"pyle's\",\n",
       " 6704: 'shootout',\n",
       " 18517: \"mike's\",\n",
       " 52275: \"driscoll's\",\n",
       " 40935: 'cogsworth',\n",
       " 52276: \"britian's\",\n",
       " 34744: 'childs',\n",
       " 52277: \"portrait's\",\n",
       " 3626: 'chain',\n",
       " 2497: 'whoever',\n",
       " 52278: 'puttered',\n",
       " 52279: 'childe',\n",
       " 52280: 'maywether',\n",
       " 3036: 'chair',\n",
       " 52281: \"rance's\",\n",
       " 34745: 'machu',\n",
       " 4517: 'ballet',\n",
       " 34746: 'grapples',\n",
       " 76152: 'summerize',\n",
       " 30603: 'freelance',\n",
       " 52283: \"andrea's\",\n",
       " 52284: '\\x91very',\n",
       " 45879: 'coolidge',\n",
       " 18518: 'mache',\n",
       " 52285: 'balled',\n",
       " 40937: 'grappled',\n",
       " 18519: 'macha',\n",
       " 21921: 'underlining',\n",
       " 5623: 'macho',\n",
       " 19507: 'oversight',\n",
       " 25257: 'machi',\n",
       " 11311: 'verbally',\n",
       " 21922: 'tenacious',\n",
       " 40938: 'windshields',\n",
       " 18557: 'paychecks',\n",
       " 3396: 'jerk',\n",
       " 11931: \"good'\",\n",
       " 34748: 'prancer',\n",
       " 21923: 'prances',\n",
       " 52286: 'olympus',\n",
       " 21924: 'lark',\n",
       " 10785: 'embark',\n",
       " 7365: 'gloomy',\n",
       " 52287: 'jehaan',\n",
       " 52288: 'turaqui',\n",
       " 20607: \"child'\",\n",
       " 2894: 'locked',\n",
       " 52289: 'pranced',\n",
       " 2588: 'exact',\n",
       " 52290: 'unattuned',\n",
       " 783: 'minute',\n",
       " 16118: 'skewed',\n",
       " 40940: 'hodgins',\n",
       " 34749: 'skewer',\n",
       " 52291: 'think\\x85',\n",
       " 38765: 'rosenstein',\n",
       " 52292: 'helmit',\n",
       " 34750: 'wrestlemanias',\n",
       " 16826: 'hindered',\n",
       " 30604: \"martha's\",\n",
       " 52293: 'cheree',\n",
       " 52294: \"pluckin'\",\n",
       " 40941: 'ogles',\n",
       " 11932: 'heavyweight',\n",
       " 82190: 'aada',\n",
       " 11312: 'chopping',\n",
       " 61534: 'strongboy',\n",
       " 41342: 'hegemonic',\n",
       " 40942: 'adorns',\n",
       " 41346: 'xxth',\n",
       " 34751: 'nobuhiro',\n",
       " 52298: 'capitães',\n",
       " 52299: 'kavogianni',\n",
       " 13422: 'antwerp',\n",
       " 6538: 'celebrated',\n",
       " 52300: 'roarke',\n",
       " 40943: 'baggins',\n",
       " 31270: 'cheeseburgers',\n",
       " 52301: 'matras',\n",
       " 52302: \"nineties'\",\n",
       " 52303: \"'craig'\",\n",
       " 12999: 'celebrates',\n",
       " 3383: 'unintentionally',\n",
       " 14362: 'drafted',\n",
       " 52304: 'climby',\n",
       " 52305: '303',\n",
       " 18520: 'oldies',\n",
       " 9096: 'climbs',\n",
       " 9655: 'honour',\n",
       " 34752: 'plucking',\n",
       " 30074: '305',\n",
       " 5514: 'address',\n",
       " 40944: 'menjou',\n",
       " 42592: \"'freak'\",\n",
       " 19508: 'dwindling',\n",
       " 9458: 'benson',\n",
       " 52307: 'white’s',\n",
       " 40945: 'shamelessness',\n",
       " 21925: 'impacted',\n",
       " 52308: 'upatz',\n",
       " 3840: 'cusack',\n",
       " 37567: \"flavia's\",\n",
       " 52309: 'effette',\n",
       " 34753: 'influx',\n",
       " 52310: 'boooooooo',\n",
       " 52311: 'dimitrova',\n",
       " 13423: 'houseman',\n",
       " 25259: 'bigas',\n",
       " 52312: 'boylen',\n",
       " 52313: 'phillipenes',\n",
       " 40946: 'fakery',\n",
       " 27658: \"grandpa's\",\n",
       " 27659: 'darnell',\n",
       " 19509: 'undergone',\n",
       " 52315: 'handbags',\n",
       " 21926: 'perished',\n",
       " 37778: 'pooped',\n",
       " 27660: 'vigour',\n",
       " 3627: 'opposed',\n",
       " 52316: 'etude',\n",
       " 11799: \"caine's\",\n",
       " 52317: 'doozers',\n",
       " 34754: 'photojournals',\n",
       " 52318: 'perishes',\n",
       " 34755: 'constrains',\n",
       " 40948: 'migenes',\n",
       " 30605: 'consoled',\n",
       " 16827: 'alastair',\n",
       " 52319: 'wvs',\n",
       " 52320: 'ooooooh',\n",
       " 34756: 'approving',\n",
       " 40949: 'consoles',\n",
       " 52064: 'disparagement',\n",
       " 52322: 'futureistic',\n",
       " 52323: 'rebounding',\n",
       " 52324: \"'date\",\n",
       " 52325: 'gregoire',\n",
       " 21927: 'rutherford',\n",
       " 34757: 'americanised',\n",
       " 82196: 'novikov',\n",
       " 1042: 'following',\n",
       " 34758: 'munroe',\n",
       " 52326: \"morita'\",\n",
       " 52327: 'christenssen',\n",
       " 23106: 'oatmeal',\n",
       " 25260: 'fossey',\n",
       " 40950: 'livered',\n",
       " 13000: 'listens',\n",
       " 76164: \"'marci\",\n",
       " 52330: \"otis's\",\n",
       " 23387: 'thanking',\n",
       " 16019: 'maude',\n",
       " 34759: 'extensions',\n",
       " 52332: 'ameteurish',\n",
       " 52333: \"commender's\",\n",
       " 27661: 'agricultural',\n",
       " 4518: 'convincingly',\n",
       " 17639: 'fueled',\n",
       " 54014: 'mahattan',\n",
       " 40952: \"paris's\",\n",
       " 52336: 'vulkan',\n",
       " 52337: 'stapes',\n",
       " 52338: 'odysessy',\n",
       " 12259: 'harmon',\n",
       " 4252: 'surfing',\n",
       " 23494: 'halloran',\n",
       " 49580: 'unbelieveably',\n",
       " 52339: \"'offed'\",\n",
       " 30607: 'quadrant',\n",
       " 19510: 'inhabiting',\n",
       " 34760: 'nebbish',\n",
       " 40953: 'forebears',\n",
       " 34761: 'skirmish',\n",
       " 52340: 'ocassionally',\n",
       " 52341: \"'resist\",\n",
       " 21928: 'impactful',\n",
       " 52342: 'spicier',\n",
       " 40954: 'touristy',\n",
       " 52343: \"'football'\",\n",
       " 40955: 'webpage',\n",
       " 52345: 'exurbia',\n",
       " 52346: 'jucier',\n",
       " 14901: 'professors',\n",
       " 34762: 'structuring',\n",
       " 30608: 'jig',\n",
       " 40956: 'overlord',\n",
       " 25261: 'disconnect',\n",
       " 82201: 'sniffle',\n",
       " 40957: 'slimeball',\n",
       " 40958: 'jia',\n",
       " 16828: 'milked',\n",
       " 40959: 'banjoes',\n",
       " 1237: 'jim',\n",
       " 52348: 'workforces',\n",
       " 52349: 'jip',\n",
       " 52350: 'rotweiller',\n",
       " 34763: 'mundaneness',\n",
       " 52351: \"'ninja'\",\n",
       " 11040: \"dead'\",\n",
       " 40960: \"cipriani's\",\n",
       " 20608: 'modestly',\n",
       " 52352: \"professor'\",\n",
       " 40961: 'shacked',\n",
       " 34764: 'bashful',\n",
       " 23388: 'sorter',\n",
       " 16120: 'overpowering',\n",
       " 18521: 'workmanlike',\n",
       " 27662: 'henpecked',\n",
       " 18522: 'sorted',\n",
       " 52354: \"jōb's\",\n",
       " 52355: \"'always\",\n",
       " 34765: \"'baptists\",\n",
       " 52356: 'dreamcatchers',\n",
       " 52357: \"'silence'\",\n",
       " 21929: 'hickory',\n",
       " 52358: 'fun\\x97yet',\n",
       " 52359: 'breakumentary',\n",
       " 15496: 'didn',\n",
       " 52360: 'didi',\n",
       " 52361: 'pealing',\n",
       " 40962: 'dispite',\n",
       " 25262: \"italy's\",\n",
       " 21930: 'instability',\n",
       " 6539: 'quarter',\n",
       " 12608: 'quartet',\n",
       " 52362: 'padmé',\n",
       " 52363: \"'bleedmedry\",\n",
       " 52364: 'pahalniuk',\n",
       " 52365: 'honduras',\n",
       " 10786: 'bursting',\n",
       " 41465: \"pablo's\",\n",
       " 52367: 'irremediably',\n",
       " 40963: 'presages',\n",
       " 57832: 'bowlegged',\n",
       " 65183: 'dalip',\n",
       " 6260: 'entering',\n",
       " 76172: 'newsradio',\n",
       " 54150: 'presaged',\n",
       " 27663: \"giallo's\",\n",
       " 40964: 'bouyant',\n",
       " 52368: 'amerterish',\n",
       " 18523: 'rajni',\n",
       " 30610: 'leeves',\n",
       " 34767: 'macauley',\n",
       " 612: 'seriously',\n",
       " 52369: 'sugercoma',\n",
       " 52370: 'grimstead',\n",
       " 52371: \"'fairy'\",\n",
       " 30611: 'zenda',\n",
       " 52372: \"'twins'\",\n",
       " 17640: 'realisation',\n",
       " 27664: 'highsmith',\n",
       " 7817: 'raunchy',\n",
       " 40965: 'incentives',\n",
       " 52374: 'flatson',\n",
       " 35097: 'snooker',\n",
       " 16829: 'crazies',\n",
       " 14902: 'crazier',\n",
       " 7094: 'grandma',\n",
       " 52375: 'napunsaktha',\n",
       " 30612: 'workmanship',\n",
       " 52376: 'reisner',\n",
       " 61306: \"sanford's\",\n",
       " 52377: '\\x91doña',\n",
       " 6108: 'modest',\n",
       " 19153: \"everything's\",\n",
       " 40966: 'hamer',\n",
       " 52379: \"couldn't'\",\n",
       " 13001: 'quibble',\n",
       " 52380: 'socking',\n",
       " 21931: 'tingler',\n",
       " 52381: 'gutman',\n",
       " 40967: 'lachlan',\n",
       " 52382: 'tableaus',\n",
       " 52383: 'headbanger',\n",
       " 2847: 'spoken',\n",
       " 34768: 'cerebrally',\n",
       " 23490: \"'road\",\n",
       " 21932: 'tableaux',\n",
       " 40968: \"proust's\",\n",
       " 40969: 'periodical',\n",
       " 52385: \"shoveller's\",\n",
       " 25263: 'tamara',\n",
       " 17641: 'affords',\n",
       " 3249: 'concert',\n",
       " 87955: \"yara's\",\n",
       " 52386: 'someome',\n",
       " 8424: 'lingering',\n",
       " 41511: \"abraham's\",\n",
       " 34769: 'beesley',\n",
       " 34770: 'cherbourg',\n",
       " 28624: 'kagan',\n",
       " 9097: 'snatch',\n",
       " 9260: \"miyazaki's\",\n",
       " 25264: 'absorbs',\n",
       " 40970: \"koltai's\",\n",
       " 64027: 'tingled',\n",
       " 19511: 'crossroads',\n",
       " 16121: 'rehab',\n",
       " 52389: 'falworth',\n",
       " 52390: 'sequals',\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-relations",
   "metadata": {},
   "source": [
    "### Split Data into Training and Test Data\n",
    "Data is already splitted into training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-degree",
   "metadata": {},
   "source": [
    "### Build the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "satellite-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_vocab_size = 10000 #Vocablury size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-audio",
   "metadata": {},
   "source": [
    "### Pad Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "removed-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define maximum number of words to consider in each review\n",
    "max_review_length = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "advisory-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad training and test reviews\n",
    "training_data = tf.keras.preprocessing.sequence.pad_sequences(training_data.tolist(),\n",
    "                                                        maxlen=max_review_length,\n",
    "                                                        padding='pre', truncating='post')\n",
    "testing_data = tf.keras.preprocessing.sequence.pad_sequences(testing_data.tolist(), \n",
    "                                                       maxlen=max_review_length, \n",
    "                                                       padding='pre', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "extended-mitchell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data[5]), len(testing_data[5])    # checking the lenght of review after padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "solar-steel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    1,  778,  128,   74,   12,  630,  163,\n",
       "         15,    4, 1766, 7982, 1051,    2,   32,   85,  156,   45,   40,\n",
       "        148,  139,  121,  664,  665,   10,   10, 1361,  173,    4,  749,\n",
       "          2,   16, 3804,    8,    4,  226,   65,   12,   43,  127,   24,\n",
       "          2,   10,   10])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "vertical-toyota",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 300), (25000, 300))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape, testing_data.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-blocking",
   "metadata": {},
   "source": [
    "### Build the Graph using RNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "accepting-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize model\n",
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-vinyl",
   "metadata": {
    "id": "DF_wJp6sKMRv"
   },
   "source": [
    "Add Embedding layer\n",
    " - Embedding Layer Input = Batch_Size * Length of each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "boolean-corner",
   "metadata": {
    "id": "iignS5XqKMRv"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Embedding(desired_vocab_size + 1, #Vocablury size\n",
    "                                    50, #Embedding size\n",
    "                                    input_length=max_review_length) #Number of words in each review\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "german-spray",
   "metadata": {
    "id": "9esFq2mNZfFZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding/embedding_lookup/Identity_1:0' shape=(None, 300, 50) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-tribute",
   "metadata": {
    "id": "FyKoymftKMRz"
   },
   "source": [
    "Embedding Layer Output - \n",
    "[Batch_Size , Review Length , Embedding_Size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-organ",
   "metadata": {
    "id": "d1Y5Yh8gKMR0"
   },
   "source": [
    "Add LSTM Layer with 256 as RNN state size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "lesbian-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "model.add(tf.keras.layers.LSTM(256)) #RNN State - size of cell state and hidden state\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "thirty-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "hazardous-appeal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 50)           500050    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300, 50)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               314368    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 814,675\n",
      "Trainable params: 814,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "objective-identity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 428s 548ms/step - loss: 0.5038 - accuracy: 0.7541 - val_loss: 0.4098 - val_accuracy: 0.8258\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 428s 548ms/step - loss: 0.3791 - accuracy: 0.8402 - val_loss: 0.3872 - val_accuracy: 0.8470\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 423s 541ms/step - loss: 0.3001 - accuracy: 0.8819 - val_loss: 0.3501 - val_accuracy: 0.8561\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 432s 553ms/step - loss: 0.2288 - accuracy: 0.9150 - val_loss: 0.3497 - val_accuracy: 0.8587\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 439s 562ms/step - loss: 0.1843 - accuracy: 0.9308 - val_loss: 0.3896 - val_accuracy: 0.8567\n"
     ]
    }
   ],
   "source": [
    "result = model.fit(training_data,training_targets,\n",
    "          epochs=5,\n",
    "          batch_size=32,          \n",
    "          validation_data=(testing_data, testing_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-jamaica",
   "metadata": {},
   "source": [
    "### Simple LSTM Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "protecting-wisdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 114s 145ms/step - loss: 0.1047 - accuracy: 0.9658\n",
      "Training loss is: 0.10468510538339615\n",
      "Training accuracy is: 0.9657999873161316\n",
      "782/782 [==============================] - 111s 141ms/step - loss: 0.3896 - accuracy: 0.8567\n",
      "Test loss is: 0.3895992934703827\n",
      "Test accuracy is: 0.856719970703125\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "score, acc = model.evaluate(training_data, training_targets, batch_size=32)\n",
    "print(\"Training loss is:\", score)\n",
    "print('Training accuracy is:', acc)\n",
    "\n",
    "# Testing data\n",
    "score, acc = model.evaluate(testing_data, testing_targets, batch_size=32)\n",
    "print(\"Test loss is:\", score)\n",
    "print('Test accuracy is:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-worse",
   "metadata": {},
   "source": [
    "### Generate predictions from Simple LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "broke-soccer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate predictions for 5 samples\n",
      "Actual Sentiment: [0 1 1 0 1]\n",
      "Predicted Sentiment: [0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Generate predictions for 5 samples\")\n",
    "probability_predictions = model.predict(testing_data[:5])\n",
    "print(\"Actual Sentiment:\", testing_targets[:5])\n",
    "\n",
    "predictions = []\n",
    "for i in probability_predictions:\n",
    "    if i>=0.5:      # taking threshold as middle value\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "        \n",
    "print(\"Predicted Sentiment:\", predictions)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-tension",
   "metadata": {},
   "source": [
    "We can use transfer learning using Glove and Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-jacket",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-bread",
   "metadata": {},
   "source": [
    "### Download Google Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fossil-grenada",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t9fS8hLYCLiD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googledrivedownloader in c:\\users\\vaibh\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install googledrivedownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "personalized-crash",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eOTPKU69CM5_"
   },
   "outputs": [],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='0B7XkCwpI5KDYNlNUTTlSS21pQmM',\n",
    "                                    dest_path='./GoogleNews-vectors-negative300.bin.gz',\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "regulated-cooling",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHCMZ5kzCQ4A"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "lasting-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('GoogleNews-vectors-negative300.bin.gz', 'rb') as f_in:\n",
    "    with open('GoogleNews-vectors-negative300.bin', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-domestic",
   "metadata": {},
   "source": [
    "##### Get Pre-trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "changed-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cardiac-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "# Load pretrained model\n",
    "model_word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "brown-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.keras.preprocessing.text.Tokenizer(num_words=desired_vocab_size, oov_token=32) # num_words -> Vocablury size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "rotary-galaxy",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SbgVAzAPDl1O"
   },
   "outputs": [],
   "source": [
    "embedding_vector_length = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "flying-insured",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPNcZC9PEA8v"
   },
   "outputs": [],
   "source": [
    "#Initialize embedding matrix\n",
    "embedding_matrix = np.zeros((desired_vocab_size + 1, embedding_vector_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fixed-newport",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mufDrkM-EKlK"
   },
   "outputs": [],
   "source": [
    "#Load word vectors for each word from Google Word2Vec model\n",
    "for word, i in sorted(t.word_index.items(),key=lambda x:x[1]):\n",
    "    if i > (desired_vocab_size+1):\n",
    "        break\n",
    "    try:\n",
    "        embedding_vector = model_word2vec[word] #Reading word's embedding from Google Word2Vec\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "middle-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize model\n",
    "tf.keras.backend.clear_session()\n",
    "model_word2vec = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "varied-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec.add(tf.keras.layers.Embedding(desired_vocab_size + 1, #Vocablury size\n",
    "                                    300, #Embedding size\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    trainable=False,\n",
    "                                    input_length=max_review_length) #Number of words in each review\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "exposed-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec.add(tf.keras.layers.Dropout(0.2))\n",
    "model_word2vec.add(tf.keras.layers.LSTM(256)) #RNN State - size of cell state and hidden state\n",
    "model_word2vec.add(tf.keras.layers.Dropout(0.2))\n",
    "model_word2vec.add(tf.keras.layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "trying-parent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense/Sigmoid:0' shape=(None, 1) dtype=float32>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_word2vec.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "proper-choice",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GobXBLHXKMR9"
   },
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model_word2vec.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "judicial-allen",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iueK1G3pOznW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 300)          3000300   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               570368    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 3,570,925\n",
      "Trainable params: 570,625\n",
      "Non-trainable params: 3,000,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_word2vec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "official-duration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 624s 797ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 633s 810ms/step - loss: 0.6931 - accuracy: 0.5049 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 647s 827ms/step - loss: 0.6933 - accuracy: 0.4982 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 619s 791ms/step - loss: 0.6933 - accuracy: 0.4954 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 613s 784ms/step - loss: 0.6932 - accuracy: 0.5026 - val_loss: 0.6933 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "result_word2vec = model_word2vec.fit(training_data,training_targets,\n",
    "          epochs=5,\n",
    "          batch_size=32,          \n",
    "          validation_data=(testing_data, testing_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-cornell",
   "metadata": {},
   "source": [
    "### Word2Vec Embeddings Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "three-watson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 179s 229ms/step - loss: 0.6933 - accuracy: 0.5000\n",
      "Training loss is: 0.693271279335022\n",
      "Training accuracy is: 0.5\n",
      "782/782 [==============================] - 174s 222ms/step - loss: 0.6933 - accuracy: 0.5000\n",
      "Test loss is: 0.6932714581489563\n",
      "Test accuracy is: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "score, acc = model_word2vec.evaluate(training_data, training_targets, batch_size=32)\n",
    "print(\"Training loss is:\", score)\n",
    "print('Training accuracy is:', acc)\n",
    "\n",
    "# Testing data\n",
    "score, acc = model_word2vec.evaluate(testing_data, testing_targets, batch_size=32)\n",
    "print(\"Test loss is:\", score)\n",
    "print('Test accuracy is:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-sunrise",
   "metadata": {},
   "source": [
    "### Generate predictions from Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "outstanding-chambers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate predictions for 5 samples\n",
      "Actual Sentiment: [0 1 1 0 1]\n",
      "Predicted Sentiment: [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Generate predictions for 5 samples\")\n",
    "probability_predictions = model_word2vec.predict(testing_data[:5])\n",
    "print(\"Actual Sentiment:\", testing_targets[:5])\n",
    "\n",
    "predictions = []\n",
    "for i in probability_predictions:\n",
    "    if i>=0.5:      # taking threshold as middle value\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "        \n",
    "print(\"Predicted Sentiment:\", predictions)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-standard",
   "metadata": {},
   "source": [
    "## Glove Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fourth-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "known-lewis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 300)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Glove file - we are using model with 50 embedding size\n",
    "glove_input_file = 'glove.6B.300d.txt'\n",
    "\n",
    "#Name for word2vec file\n",
    "word2vec_output_file = 'glove.6B.300d.txt.word2vec'\n",
    "\n",
    "#Convert Glove embeddings to Word2Vec embeddings\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-farming",
   "metadata": {
    "id": "UTZqcuaqB2cl"
   },
   "source": [
    "### Get Embeddings (from Pre-trained model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-indonesian",
   "metadata": {
    "id": "sCVQv-FgtrCp"
   },
   "source": [
    "Pre-trained Glove model has 400,000 unique words (Vocabulary size). We do not need all the words. Moreover, we have to arrange word embeddings according to word index created by our tokenizers above. So we will extract word embeddings for only the words that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "antique-comfort",
   "metadata": {
    "id": "0yR0q8uz5ZC8"
   },
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "thrown-guinea",
   "metadata": {
    "id": "OiCu6QGz5jJA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 300)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Glove file - we are using model with 50 embedding size\n",
    "glove_input_file = 'glove.6B.300d.txt'\n",
    "\n",
    "#Name for word2vec file\n",
    "word2vec_output_file = 'glove.6B.300d.txt.word2vec'\n",
    "\n",
    "#Convert Glove embeddings to Word2Vec embeddings\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "parliamentary-independence",
   "metadata": {
    "id": "o1ycexOAB4qD"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "treated-montreal",
   "metadata": {
    "id": "tnfmU0U7B1ja"
   },
   "outputs": [],
   "source": [
    "# Load pretrained Glove model (in word2vec form)\n",
    "model_glove = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "continuous-champion",
   "metadata": {
    "id": "SbgVAzAPDl1O"
   },
   "outputs": [],
   "source": [
    "#Embedding length based on selected model - we are using 300d here.\n",
    "embedding_vector_length = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-practice",
   "metadata": {
    "id": "i_OwTH8duDX0"
   },
   "source": [
    "Initialize a embedding matrix which we will populate for our vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "enormous-shore",
   "metadata": {
    "id": "HPNcZC9PEA8v"
   },
   "outputs": [],
   "source": [
    "#Initialize embedding matrix\n",
    "embedding_matrix = np.zeros((desired_vocab_size + 1, embedding_vector_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "enclosed-mission",
   "metadata": {
    "id": "sgRtfyRzi25X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10001, 300)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-certificate",
   "metadata": {
    "id": "yu0SnwlguM4e"
   },
   "source": [
    "Load word vectors for each word in our vocabulary from from Glove pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "greek-highway",
   "metadata": {
    "id": "mufDrkM-EKlK"
   },
   "outputs": [],
   "source": [
    "for word, i in sorted(t.word_index.items(),key=lambda x:x[1]):\n",
    "    if i > (desired_vocab_size+1):\n",
    "        break\n",
    "    try:\n",
    "        embedding_vector = model_glove[word] #Reading word's embedding from Glove model for a given word\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "peaceful-improvement",
   "metadata": {
    "id": "qeZNhU2ZEs1w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word the - index 1\n",
    "embedding_matrix[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-slope",
   "metadata": {
    "id": "wAOvV9C_KMRl"
   },
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "widespread-interface",
   "metadata": {
    "id": "pWtUZzM3KMRs"
   },
   "outputs": [],
   "source": [
    "#Initialize model\n",
    "tf.keras.backend.clear_session()\n",
    "model_glove = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-delicious",
   "metadata": {
    "id": "DF_wJp6sKMRv"
   },
   "source": [
    "Add Embedding layer\n",
    " - Embedding Layer Input = Batch_Size * Length of each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "recorded-prediction",
   "metadata": {
    "id": "iignS5XqKMRv"
   },
   "outputs": [],
   "source": [
    "model_glove.add(tf.keras.layers.Embedding(desired_vocab_size + 1, #Vocablury size\n",
    "                                    embedding_vector_length, #Embedding size\n",
    "                                    weights=[embedding_matrix], #Embeddings taken from pre-trained model\n",
    "                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.\n",
    "                                    input_length=max_review_length) #Number of words in each review\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "decent-security",
   "metadata": {
    "id": "9esFq2mNZfFZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding/embedding_lookup/Identity_1:0' shape=(None, 300, 300) dtype=float32>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-manhattan",
   "metadata": {
    "id": "FyKoymftKMRz"
   },
   "source": [
    "Embedding Layer Output - \n",
    "[Batch_Size , Review Length , Embedding_Size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-medication",
   "metadata": {
    "id": "d1Y5Yh8gKMR0"
   },
   "source": [
    "Add LSTM Layer with 256 as RNN state size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "emerging-socket",
   "metadata": {
    "id": "YLrE_hSGKMR2"
   },
   "outputs": [],
   "source": [
    "model_glove.add(tf.keras.layers.Dropout(0.2))\n",
    "model_glove.add(tf.keras.layers.LSTM(256)) #RNN State - size of cell state and hidden state\n",
    "model_glove.add(tf.keras.layers.Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "celtic-grenada",
   "metadata": {
    "id": "lWABotFJac4J"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_1/cond/Identity:0' shape=(None, 256) dtype=float32>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-hospital",
   "metadata": {
    "id": "xi79uySUOS5u"
   },
   "source": [
    "Use Dense layer for output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "integrated-retirement",
   "metadata": {
    "id": "MJ-o1jtUKMR6"
   },
   "outputs": [],
   "source": [
    "model_glove.add(tf.keras.layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "classified-swift",
   "metadata": {
    "id": "GobXBLHXKMR9"
   },
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model_glove.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "engaged-central",
   "metadata": {
    "id": "iueK1G3pOznW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 300)          3000300   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               570368    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 3,570,925\n",
      "Trainable params: 570,625\n",
      "Non-trainable params: 3,000,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_glove.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-apache",
   "metadata": {
    "id": "j_aH5TX5KMSA"
   },
   "source": [
    "### Train Glove Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "clinical-detroit",
   "metadata": {
    "id": "AV3TceqjKMSC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 596s 763ms/step - loss: 0.6934 - accuracy: 0.4956 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 607s 776ms/step - loss: 0.6933 - accuracy: 0.4970 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 580s 742ms/step - loss: 0.6933 - accuracy: 0.5040 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 592s 757ms/step - loss: 0.6933 - accuracy: 0.4976 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 549s 702ms/step - loss: 0.6933 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "result_glove = model_glove.fit(training_data,training_targets,\n",
    "          epochs=5,\n",
    "          batch_size=32,\n",
    "          validation_data=(testing_data, testing_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-study",
   "metadata": {},
   "source": [
    "### Glove Embeddings Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "classical-madagascar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 148s 189ms/step - loss: 0.6932 - accuracy: 0.5000\n",
      "Training loss is: 0.6931529641151428\n",
      "Training accuracy is: 0.5\n",
      "782/782 [==============================] - 160s 205ms/step - loss: 0.6932 - accuracy: 0.5000\n",
      "Test loss is: 0.6931532025337219\n",
      "Test accuracy is: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "score, acc = model_glove.evaluate(training_data, training_targets, batch_size=32)\n",
    "print(\"Training loss is:\", score)\n",
    "print('Training accuracy is:', acc)\n",
    "\n",
    "# Testing data\n",
    "score, acc = model_glove.evaluate(testing_data, testing_targets, batch_size=32)\n",
    "print(\"Test loss is:\", score)\n",
    "print('Test accuracy is:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-decade",
   "metadata": {},
   "source": [
    "## Generate predictions from Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cardiovascular-suspect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate predictions for 5 samples\n",
      "Actual Sentiment: [0 1 1 0 1]\n",
      "Predicted Sentiment: [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Generate predictions for 5 samples\")\n",
    "probability_predictions = model_glove.predict(testing_data[:5])\n",
    "print(\"Actual Sentiment:\", testing_targets[:5])\n",
    "\n",
    "predictions = []\n",
    "for i in probability_predictions:\n",
    "    if i>=0.5:      # taking threshold as middle value\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "        \n",
    "print(\"Predicted Sentiment:\", predictions)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-member",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "continent-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize model\n",
    "tf.keras.backend.clear_session()\n",
    "model_bidirectional = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "running-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bidirectional.add(tf.keras.layers.Embedding(desired_vocab_size + 1, #Vocablury size\n",
    "                                    embedding_vector_length, #Embedding size\n",
    "                                    weights=[embedding_matrix], #Embeddings taken from pre-trained model\n",
    "                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.\n",
    "                                    input_length=max_review_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "clinical-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bidirectional.add(Bidirectional(LSTM(128, return_sequences = True)))\n",
    "model_bidirectional.add(Dense(64, activation=\"relu\"))\n",
    "model_bidirectional.add(Dropout(0.5))\n",
    "model_bidirectional.add(Dense(16, activation=\"relu\"))\n",
    "model_bidirectional.add(Dropout(0.5))\n",
    "# model_bidirectional.add(Dense(32, activation=\"relu\"))\n",
    "# model_bidirectional.add(Dropout(0.4))\n",
    "model_bidirectional.add(Dense(1, activation=\"sigmoid\"))\n",
    "model_bidirectional.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "increasing-provision",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_2/Sigmoid:0' shape=(None, 300, 1) dtype=float32>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bidirectional.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "irish-prairie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 50)           500050    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 300, 256)          183296    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300, 64)           16448     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300, 16)           1040      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300, 16)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300, 1)            17        \n",
      "=================================================================\n",
      "Total params: 700,851\n",
      "Trainable params: 200,801\n",
      "Non-trainable params: 500,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bidirectional.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-chase",
   "metadata": {},
   "source": [
    "### Train Bidirectional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "welsh-lending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 391s 500ms/step - loss: 0.6934 - accuracy: 0.5045 - val_loss: 0.6931 - val_accuracy: 0.5041\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 383s 490ms/step - loss: 0.6919 - accuracy: 0.5154 - val_loss: 0.6842 - val_accuracy: 0.5376\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 376s 481ms/step - loss: 0.6879 - accuracy: 0.5385 - val_loss: 0.6784 - val_accuracy: 0.5639\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 375s 480ms/step - loss: 0.6411 - accuracy: 0.6417 - val_loss: 0.6261 - val_accuracy: 0.6543\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 370s 473ms/step - loss: 0.6004 - accuracy: 0.6861 - val_loss: 0.5717 - val_accuracy: 0.6996\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 378s 484ms/step - loss: 0.5605 - accuracy: 0.7210 - val_loss: 0.5308 - val_accuracy: 0.7254\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 374s 479ms/step - loss: 0.5200 - accuracy: 0.7536 - val_loss: 0.5220 - val_accuracy: 0.7395\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 377s 483ms/step - loss: 0.4824 - accuracy: 0.7787 - val_loss: 0.4958 - val_accuracy: 0.7596\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 373s 477ms/step - loss: 0.4406 - accuracy: 0.8038 - val_loss: 0.4904 - val_accuracy: 0.7700\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 363s 464ms/step - loss: 0.4050 - accuracy: 0.8242 - val_loss: 0.4781 - val_accuracy: 0.7893\n"
     ]
    }
   ],
   "source": [
    "result_bidirectional = model_bidirectional.fit(training_data,training_targets,\n",
    "          epochs=10,\n",
    "          batch_size=32,          \n",
    "          validation_data=(testing_data, testing_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-calgary",
   "metadata": {},
   "source": [
    "### Bidirectional Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "13535251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 165s 211ms/step - loss: 0.3224 - accuracy: 0.8613\n",
      "Training loss is: 0.3224371373653412\n",
      "Training accuracy is: 0.8612843751907349\n",
      "782/782 [==============================] - 165s 211ms/step - loss: 0.4781 - accuracy: 0.7893\n",
      "Test loss is: 0.4780578911304474\n",
      "Test accuracy is: 0.7893074750900269\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "score, acc = model_bidirectional.evaluate(training_data, training_targets, batch_size=32)\n",
    "print(\"Training loss is:\", score)\n",
    "print('Training accuracy is:', acc)\n",
    "\n",
    "# Testing data\n",
    "score, acc = model_bidirectional.evaluate(testing_data, testing_targets, batch_size=32)\n",
    "print(\"Test loss is:\", score)\n",
    "print('Test accuracy is:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1491ed68",
   "metadata": {},
   "source": [
    "## Generate predictions from Bidirectional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "78076df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate predictions for 5 samples\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021D05D0BB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Actual Sentiment: [0 1 1 0 1]\n",
      "[[[0.42315376]\n",
      "  [0.43027514]\n",
      "  [0.43443483]\n",
      "  ...\n",
      "  [0.12200108]\n",
      "  [0.1592671 ]\n",
      "  [0.12225032]]\n",
      "\n",
      " [[0.96915287]\n",
      "  [0.97026396]\n",
      "  [0.9711535 ]\n",
      "  ...\n",
      "  [0.97331214]\n",
      "  [0.9867103 ]\n",
      "  [0.9903425 ]]\n",
      "\n",
      " [[0.41279334]\n",
      "  [0.40157527]\n",
      "  [0.3991126 ]\n",
      "  ...\n",
      "  [0.95756257]\n",
      "  [0.97160673]\n",
      "  [0.9712478 ]]\n",
      "\n",
      " [[0.34416068]\n",
      "  [0.35147148]\n",
      "  [0.3558355 ]\n",
      "  ...\n",
      "  [0.9658786 ]\n",
      "  [0.9733903 ]\n",
      "  [0.9894284 ]]\n",
      "\n",
      " [[0.9994641 ]\n",
      "  [0.99948215]\n",
      "  [0.9994974 ]\n",
      "  ...\n",
      "  [0.99908376]\n",
      "  [0.99474204]\n",
      "  [0.9834581 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Generate predictions for 5 samples\")\n",
    "probability_predictions = model_bidirectional.predict(testing_data[:5])\n",
    "print(\"Actual Sentiment:\", testing_targets[:5])\n",
    "print(probability_predictions)\n",
    "# predictions = []\n",
    "# for i in probability_predictions:\n",
    "#     if i>=0.5:      # taking threshold as middle value\n",
    "#         predictions.append(1)\n",
    "#     else:\n",
    "#         predictions.append(0)\n",
    "        \n",
    "# print(\"Predicted Sentiment:\", predictions)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-trailer",
   "metadata": {},
   "source": [
    "## Conlcusion: We could see out of all Simple LSTM and Bidirectional is providing the best prediction results. \n",
    "There could be further improvements like changing dropuouts and adding more hidden layers. However, due to time\n",
    "constraints, we could consider this as the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-racing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-delhi",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
